{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: API details.\n",
    "output-file: losses.html\n",
    "title: Losses\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| default_exp losses\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "import os, math, numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MMD_loss(nn.Module):\n",
    "    '''\n",
    "    https://github.com/ZongxianLee/MMD_Loss.Pytorch/blob/master/mmd_loss.py\n",
    "    '''\n",
    "    def __init__(self, kernel_mul = 2.0, kernel_num = 5):\n",
    "        super(MMD_loss, self).__init__()\n",
    "        self.kernel_num = kernel_num\n",
    "        self.kernel_mul = kernel_mul\n",
    "        self.fix_sigma = None\n",
    "        return\n",
    "    \n",
    "    def guassian_kernel(self, source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
    "        n_samples = int(source.size()[0])+int(target.size()[0])\n",
    "        total = torch.cat([source, target], dim=0)\n",
    "        total0 = total.unsqueeze(0).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        total1 = total.unsqueeze(1).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        L2_distance = ((total0-total1)**2).sum(2) \n",
    "        if fix_sigma:\n",
    "            bandwidth = fix_sigma\n",
    "        else:\n",
    "            bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)\n",
    "        bandwidth /= kernel_mul ** (kernel_num // 2)\n",
    "        bandwidth_list = [bandwidth * (kernel_mul**i) for i in range(kernel_num)]\n",
    "        kernel_val = [torch.exp(-L2_distance / bandwidth_temp) for bandwidth_temp in bandwidth_list]\n",
    "        return sum(kernel_val)\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        batch_size = int(source.size()[0])\n",
    "        kernels = self.guassian_kernel(source, target, kernel_mul=self.kernel_mul, kernel_num=self.kernel_num, fix_sigma=self.fix_sigma)\n",
    "        XX = kernels[:batch_size, :batch_size]\n",
    "        YY = kernels[batch_size:, batch_size:]\n",
    "        XY = kernels[:batch_size, batch_size:]\n",
    "        YX = kernels[batch_size:, :batch_size]\n",
    "        loss = torch.mean(XX + YY - XY -YX)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "import ot\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "class OT_loss(nn.Module):\n",
    "    _valid = 'emd sinkhorn sinkhorn_knopp_unbalanced'.split()\n",
    "\n",
    "    def __init__(self, which='emd', use_cuda=True, reg=0.5, reg_m=0.5):\n",
    "        if which not in self._valid:\n",
    "            raise ValueError(f'{which} not known ({self._valid})')\n",
    "        elif which == 'emd':\n",
    "            self.fn = lambda m, n, M: ot.emd(m, n, M)\n",
    "        elif which == 'sinkhorn':\n",
    "            self.fn = lambda m, n, M : ot.sinkhorn(m, n, M, reg) # maybe use 0.5 for regularization.\n",
    "        elif which == 'sinkhorn_knopp_unbalanced':\n",
    "            self.fn = lambda m, n, M : ot.unbalanced.sinkhorn_knopp_unbalanced(m, n, M, reg, reg_m) # keep in mind these regularization parameters and maybe test if they converge.\n",
    "        else:\n",
    "            pass\n",
    "        self.use_cuda=use_cuda\n",
    "\n",
    "    def __call__(self, source, target, use_cuda=None):\n",
    "        if use_cuda is None:\n",
    "            use_cuda = self.use_cuda\n",
    "        mu = torch.from_numpy(ot.unif(source.size()[0]))\n",
    "        nu = torch.from_numpy(ot.unif(target.size()[0]))\n",
    "        M = torch.cdist(source, target)**2\n",
    "        pi = self.fn(mu, nu, M.detach().cpu())\n",
    "        if type(pi) is np.ndarray:\n",
    "            pi = torch.tensor(pi)\n",
    "        elif type(pi) is torch.Tensor:\n",
    "            pi = pi.clone().detach()\n",
    "        pi = pi.cuda() if use_cuda else pi\n",
    "        M = M.to(pi.device)\n",
    "        loss = torch.sum(pi * M)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "import ot\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "class UOT_loss(nn.Module):\n",
    "    _valid = 'mm_unbalanced'.split()\n",
    "\n",
    "    def __init__(self, which='mm_unbalanced', use_cuda=True, reg_m_l2=[5., 5.], detach_M=False, detach_m=False, use_uniform=False, marginal_match_dim=1):\n",
    "        if which not in self._valid:\n",
    "            raise ValueError(f'{which} not known ({self._valid})')\n",
    "        elif which == 'mm_unbalanced':\n",
    "            self.fn = lambda m, n, M: ot.unbalanced.mm_unbalanced(m, n, M, reg_m_l2, div='l2')\n",
    "        else:\n",
    "            pass\n",
    "        self.detach_M = detach_M\n",
    "        self.detach_m = detach_m\n",
    "        self.use_uniform = use_uniform\n",
    "        assert marginal_match_dim in [0, 1]\n",
    "        self.marginal_match_dim = marginal_match_dim\n",
    "\n",
    "    def __call__(self, source, target, source_density, target_density):\n",
    "        assert source_density.shape[0] == source.shape[0] and target_density.shape[0] == target.shape[0]\n",
    "        mu = source_density\n",
    "        nu = target_density\n",
    "        M = ot.dist(source, target)\n",
    "        if self.detach_M:\n",
    "            M1 = M.detach()\n",
    "        else:\n",
    "            M1 = M\n",
    "        if self.detach_m:\n",
    "            mu1 = mu.detach()\n",
    "            nu1 = nu.detach()\n",
    "        else:\n",
    "            mu1 = mu\n",
    "            nu1 = nu\n",
    "        if self.use_uniform:\n",
    "            mu1 = target_density # the target is uniform times n_samples, and we match that.\n",
    "        plan = self.fn(mu1, nu1, M1)\n",
    "        ot_loss = (plan * M).sum()\n",
    "        marginal_transported = plan.sum(dim=self.marginal_match_dim)\n",
    "        marginal_loss = nn.functional.mse_loss(marginal_transported, mu)\n",
    "        self.plan = plan\n",
    "        return ot_loss, marginal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "\n",
    "def normalize_by_sum(tensor, dim=-1, eps=1e-8):\n",
    "    sum_along_dim = torch.sum(tensor, dim=dim, keepdim=True)\n",
    "    sum_along_dim = sum_along_dim + eps  # Add epsilon to avoid division by zero\n",
    "    return tensor / sum_along_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch.nn.functional as F\n",
    "hard_softmax = F.softmax\n",
    "# def hard_softmax(input, dim=None):\n",
    "#     zero_mask = (input.sum(dim=dim, keepdim=True) == 0)\n",
    "#     input = torch.where(zero_mask, torch.ones_like(input), input)\n",
    "#     sum_hardsigmoid = torch.sum(input, dim=dim, keepdim=True)\n",
    "#     softmax_output = input / sum_hardsigmoid\n",
    "    \n",
    "#     return softmax_output\n",
    "\n",
    "# def hard_softmax(input, dim=None, expand_factor=1., shift=0., func=lambda x: x, threshold=None):\n",
    "#     # func = F.hardsigmoid\n",
    "#     # Apply hardsigmoid to the input\n",
    "#     if expand_factor != 1.:\n",
    "#         input = input * expand_factor\n",
    "#     if shift != 0.:\n",
    "#         input = input + shift\n",
    "#     hardsigmoid_input = func(input)\n",
    "\n",
    "#     if threshold is not None:\n",
    "#         hardsigmoid_input = torch.where(hardsigmoid_input < threshold, torch.zeros_like(hardsigmoid_input), hardsigmoid_input)\n",
    "\n",
    "#     # Identify rows/columns that are all zeros\n",
    "#     zero_mask = (hardsigmoid_input.sum(dim=dim, keepdim=True) == 0)\n",
    "    \n",
    "#     # For rows/columns that are all zeros, set all values to 1\n",
    "#     hardsigmoid_input = torch.where(zero_mask, torch.ones_like(hardsigmoid_input), hardsigmoid_input)\n",
    "    \n",
    "#     # Normalize by the sum along the specified dimension\n",
    "#     sum_hardsigmoid = torch.sum(hardsigmoid_input, dim=dim, keepdim=True)\n",
    "#     softmax_output = hardsigmoid_input / sum_hardsigmoid\n",
    "    \n",
    "#     return softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "import ot\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "class density_specified_OT_loss(nn.Module):\n",
    "    \"\"\"_summary_\n",
    "    Only allowing balanced Sinkhorn, because it is the only one with stable backprop.\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "\n",
    "    Raises:\n",
    "        ValueError: _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # _valid = 'emd sinkhorn sinkhorn_knopp_unbalanced'.split()\n",
    "    _valid = 'sinkhorn'.split()\n",
    "\n",
    "    def __init__(self, which='sinkhorn', reg=0.1, reg_m=1.0, take_softmax=True, detach_M_ot=False, use_hard_softmax=True, threshold=None):\n",
    "        if which not in self._valid:\n",
    "            raise ValueError(f'{which} not known ({self._valid})')\n",
    "        elif which == 'sinkhorn':\n",
    "            self.fn = lambda m, n, M : ot.sinkhorn2(m, n, M, reg) # maybe use 0.5 for regularization.\n",
    "        # elif which == 'sinkhorn_knopp_unbalanced':\n",
    "        #     self.fn = lambda m, n, M : (ot.unbalanced.sinkhorn_knopp_unbalanced(m, n, M, reg, reg_m) * M).sum() # keep in mind these regularization parameters and maybe test if they converge.\n",
    "        #     print(\"WARNING: ot.unbalanced.sinkhorn_knopp_unbalanced chosen -- Can it back propagate density?\")\n",
    "        # elif which == 'emd':\n",
    "        #     self.fn = lambda m, n, M: ot.emd2(m, n, M)\n",
    "        #     print(\"WARNING: ot.emd chosen -- Can it back propagate density?\")\n",
    "        else:\n",
    "            pass\n",
    "        if take_softmax:\n",
    "            if use_hard_softmax:\n",
    "                self.softmax = lambda x: hard_softmax(x, dim=-1)\n",
    "            else:\n",
    "                self.softmax = lambda x: nn.functional.softmax(x, dim=-1)\n",
    "        else:\n",
    "            self.softmax = lambda x: x\n",
    "        self.detach_M_ot = detach_M_ot\n",
    "\n",
    "    def __call__(self, source, target, source_density, target_density=None):\n",
    "        # mu = torch.from_numpy(ot.unif(source.size()[0]))\n",
    "        assert source_density.shape[0] == source.shape[0]\n",
    "        mu = source_density\n",
    "        if target_density is None:\n",
    "            nu = torch.tensor(ot.unif(target.size()[0]), device=source.device, dtype=source.dtype)\n",
    "        else:\n",
    "            assert target_density.shape[0] == target.shape[0]\n",
    "            nu = target_density\n",
    "        mu = self.softmax(mu)\n",
    "        nu = self.softmax(nu)\n",
    "        M = torch.cdist(source, target)#**2 # strangely this squaring prevents backprop through the density. \n",
    "        if self.detach_M_ot:\n",
    "            M = M.detach()\n",
    "        loss = self.fn(mu, nu, M)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "class Density_loss(nn.Module):\n",
    "    def __init__(self, hinge_value=0.01, use_softmax=False, use_hard_softmax=False):\n",
    "        self.hinge_value = hinge_value\n",
    "        self.use_softmax = use_softmax\n",
    "        if use_softmax:\n",
    "            if use_hard_softmax:\n",
    "                self.softmax = lambda x: hard_softmax(x, dim=-1)\n",
    "            else:\n",
    "                self.softmax = lambda x: nn.functional.softmax(x, dim=-1)\n",
    "        else:\n",
    "            # self.softmax = lambda x: x / x.size(-1) # normalize by batch size.\n",
    "            self.softmax = normalize_by_sum\n",
    "    def __call__(self, source, target, groups = None, to_ignore = None, top_k = 5, source_weights=None, target_weights=None):\n",
    "\n",
    "        if groups is not None:\n",
    "            # for global loss\n",
    "            c_dist_list = []\n",
    "            for i in range(1, len(groups)):\n",
    "                if groups[i] != to_ignore:\n",
    "                    if source_weights is None:\n",
    "                        source_weights_i = torch.ones(source[i].size(0), device=source[i].device, dtype=source[i].dtype)\n",
    "                    else:\n",
    "                        source_weights_i = source_weights[i]\n",
    "                    source_weights_softmax_i = self.softmax(source_weights_i) * source_weights_i.shape[-1]\n",
    "                    if target_weights is None:\n",
    "                        target_weights_i = torch.ones(target[i].size(0), device=target[i].device, dtype=target[i].dtype)\n",
    "                    else:\n",
    "                        target_weights_i = target_weights[i]\n",
    "                    target_weights_softmax_i = self.softmax(target_weights_i) * target_weights_i.shape[-1]\n",
    "                    dist = torch.cdist(source[i], target[i])\n",
    "                    weighted_dist = dist * source_weights_softmax_i.unsqueeze(-1) * target_weights_softmax_i.unsqueeze(0)\n",
    "                    c_dist_list.append(weighted_dist)\n",
    "            c_dist = torch.stack(c_dist_list)\n",
    "            # NOTE: check if this should be 1 indexed\n",
    "        else:\n",
    "            if source_weights is None:\n",
    "                source_weights = torch.ones(source.size(0), device=source.device, dtype=source.dtype)\n",
    "            source_weights_softmax = self.softmax(source_weights) * source_weights.shape[-1]\n",
    "            if target_weights is None:\n",
    "                target_weights = torch.ones(target.size(0), device=target.device, dtype=target.dtype)\n",
    "            target_weights_softmax = self.softmax(target_weights) * target_weights.shape[-1]\n",
    "            # for local loss\n",
    "            c_dist = torch.stack([\n",
    "                torch.cdist(source, target) * source_weights_softmax.unsqueeze(-1) * target_weights_softmax.unsqueeze(0)\n",
    "            ])\n",
    "        values, _ = torch.topk(c_dist, top_k, dim=2, largest=False, sorted=False)\n",
    "        values -= self.hinge_value\n",
    "        values[values<0] = 0\n",
    "        loss = torch.mean(values)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class Local_density_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, sources, targets, groups, to_ignore, top_k = 5):\n",
    "        # print(source, target)\n",
    "        # c_dist = torch.cdist(source, target) \n",
    "        c_dist = torch.stack([\n",
    "            torch.cdist(sources[i], targets[i]) \n",
    "            # NOTE: check if should be from range 1 or not.\n",
    "            for i in range(1, len(groups))\n",
    "            if groups[i] != to_ignore\n",
    "        ])\n",
    "        vals, inds = torch.topk(c_dist, top_k, dim=2, largest=False, sorted=False)\n",
    "        values = vals[inds[inds]]\n",
    "        loss = torch.mean(values)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class EnergyLossSeq(nn.Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, func, xtseq, t):\n",
    "        dxdt = torch.stack([func(t[i], xtseq[i]) for i in range(len(t))])\n",
    "        return torch.square(dxdt).mean()\n",
    "    \n",
    "class EnergyLossGrowthRateSeq(nn.Module):\n",
    "    def __init__(self, weighted=True, detach_m=False, use_softmax=False, use_hard_softmax=False):\n",
    "        self.weighted = weighted\n",
    "        self.detach_m = detach_m\n",
    "        self.use_softmax = use_softmax\n",
    "        if use_softmax:\n",
    "            if use_hard_softmax:\n",
    "                self.softmax = lambda x: hard_softmax(x, dim=-1)\n",
    "            else:\n",
    "                self.softmax = lambda x: nn.functional.softmax(x, dim=-1)\n",
    "        else:\n",
    "            # self.softmax = lambda x: x / x.size(-1)\n",
    "            self.softmax = normalize_by_sum\n",
    "\n",
    "    def __call__(self, func, xtseq, mtseq, t):\n",
    "        \"\"\"Returns the energy wrt the x and m curves separately.\n",
    "\n",
    "        Args:\n",
    "            func (_type_): _description_\n",
    "            xtseq (_type_): _description_\n",
    "            mtseq (_type_): _description_\n",
    "            t (_type_): _description_\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        xmseq = torch.cat([xtseq, mtseq.unsqueeze(-1)], dim=-1)\n",
    "        dxdt = torch.stack([func(t[i], xmseq[i]) for i in range(len(t))])\n",
    "        if self.weighted:\n",
    "            masses = self.softmax(mtseq) * mtseq.shape[-1]\n",
    "            if self.detach_m:\n",
    "                masses = masses.detach()\n",
    "            dxdt = dxdt * masses.unsqueeze(-1) # weighted by mass\n",
    "        return torch.square(dxdt[...,:-1]).mean(), torch.square(dxdt[...,-1]).mean()\n",
    "    \n",
    "class EnergyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, func, x, t): # should use the current t. i.e. t_seq[-1]\n",
    "        dxdt = func(t, x)\n",
    "        return torch.square(dxdt).mean()\n",
    "\n",
    "class EnergyLossGrowthRate(nn.Module):\n",
    "    def __init__(self, weighted=True, detach_m=False, use_softmax=False, use_hard_softmax=False):\n",
    "        self.weighted = weighted\n",
    "        self.detach_m = detach_m\n",
    "        self.use_softmax = use_softmax\n",
    "        if use_softmax:\n",
    "            if use_hard_softmax:\n",
    "                self.softmax = lambda x: hard_softmax(x, dim=-1)\n",
    "            else:\n",
    "                self.softmax = lambda x: nn.functional.softmax(x, dim=-1)\n",
    "        else:\n",
    "            # self.softmax = lambda x: x / x.size(-1)\n",
    "            self.softmax = normalize_by_sum\n",
    "\n",
    "    def __call__(self, func, x, m, t):\n",
    "        xm = torch.cat([x, m.unsqueeze(-1)], dim=-1)\n",
    "        dxdt = func(t, xm)\n",
    "        if self.weighted:\n",
    "            masses = self.softmax(m) * m.shape[-1]\n",
    "            if self.detach_m:\n",
    "                masses = masses.detach()\n",
    "            dxdt = dxdt * masses.unsqueeze(-1) # weighted by mass\n",
    "        return torch.square(dxdt[...,:-1]).mean(), torch.square(dxdt[...,-1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "# not going to penalize m velo because it is growth rate.\n",
    "class UnifVeloLossSeq(nn.Module):\n",
    "    def __init__(self, square=False, topk=None):\n",
    "        self.square = square\n",
    "        self.topk = topk\n",
    "\n",
    "    def __call__(self, func, xtseq, t):\n",
    "        dxdt = torch.stack([func(t[i], xtseq[i]) for i in range(len(t))])\n",
    "        mean_velo = func.mean_velocity(t.unsqueeze(-1))\n",
    "        norms = dxdt.norm(dim=-1)\n",
    "        if self.square:\n",
    "            res = torch.square(norms - mean_velo)\n",
    "            # return torch.square(norms - mean_velo).mean()\n",
    "        else:\n",
    "            # return torch.abs(norms - mean_velo).mean()\n",
    "            res = torch.abs(norms - mean_velo)\n",
    "        if self.topk is not None:\n",
    "            values, _ = torch.topk(res, self.topk, dim=0, largest=True, sorted=True)\n",
    "            return values.mean()\n",
    "        else:\n",
    "            return res.mean()\n",
    "    \n",
    "class UnifVeloLossGrowthRateSeq(nn.Module):\n",
    "    def __init__(self, weighted=True, detach_m=False, use_softmax=False, use_hard_softmax=False, square=False, topk=None):\n",
    "        self.weighted = weighted\n",
    "        self.detach_m = detach_m\n",
    "        self.use_softmax = use_softmax\n",
    "        if use_softmax:\n",
    "            if use_hard_softmax:\n",
    "                self.softmax = lambda x: hard_softmax(x, dim=-1)\n",
    "            else:\n",
    "                self.softmax = lambda x: nn.functional.softmax(x, dim=-1)\n",
    "        else:\n",
    "            # self.softmax = lambda x: x / x.size(-1)\n",
    "            self.softmax = normalize_by_sum\n",
    "        self.square = square\n",
    "        self.topk = topk\n",
    "    def __call__(self, func, xtseq, mtseq, t):\n",
    "        \"\"\"Returns the UnifVelo wrt the x and m curves separately.\n",
    "\n",
    "        Args:\n",
    "            func (_type_): _description_\n",
    "            xtseq (_type_): _description_\n",
    "            mtseq (_type_): _description_\n",
    "            t (_type_): _description_\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        xmseq = torch.cat([xtseq, mtseq.unsqueeze(-1)], dim=-1)\n",
    "        dxdt = torch.stack([func(t[i], xmseq[i]) for i in range(len(t))])\n",
    "        mean_velo = func.mean_velocity(t.unsqueeze(-1))\n",
    "        if self.weighted:\n",
    "            masses = self.softmax(mtseq) * mtseq.shape[-1]\n",
    "            if self.detach_m:\n",
    "                masses = masses.detach()\n",
    "            dxdt = dxdt * masses.unsqueeze(-1) # weighted by mass\n",
    "        # return torch.square(dxdt[...,:-1]).sum(axis=-1).var(), torch.square(dxdt[...,-1]).var()\n",
    "        norms = dxdt[...,:-1].norm(dim=-1)\n",
    "        if self.square:\n",
    "            res = torch.square(norms - mean_velo)\n",
    "        else:\n",
    "            res = torch.abs(norms - mean_velo)\n",
    "        if self.topk is not None:\n",
    "            values, _ = torch.topk(res, self.topk, dim=0, largest=True, sorted=True)\n",
    "            return values.mean()\n",
    "        else:\n",
    "            return res.mean()\n",
    "    \n",
    "class UnifVeloLoss(nn.Module):\n",
    "    def __init__(self, square=False, topk=None):\n",
    "        self.square = square\n",
    "        self.topk = topk\n",
    "\n",
    "    def __call__(self, func, x, t): # should use the current t. i.e. t_seq[-1]\n",
    "        dxdt = func(t, x)\n",
    "        mean_velo = func.mean_velocity(torch.tensor([t], dtype=x.dtype, device=x.device))\n",
    "        if self.square:\n",
    "            res = torch.square(dxdt.norm(dim=-1) - mean_velo)\n",
    "        else:\n",
    "            res = torch.abs(dxdt.norm(dim=-1) - mean_velo)\n",
    "        if self.topk is not None:\n",
    "            values, _ = torch.topk(res, self.topk, dim=0, largest=True, sorted=True)\n",
    "            return values.mean()\n",
    "        else:\n",
    "            return res.mean()\n",
    "\n",
    "class UnifVeloLossGrowthRate(nn.Module):\n",
    "    def __init__(self, weighted=True, detach_m=False, use_softmax=False, use_hard_softmax=False, square=False, topk=None):\n",
    "        self.weighted = weighted\n",
    "        self.detach_m = detach_m\n",
    "        self.use_softmax = use_softmax\n",
    "        if use_softmax:\n",
    "            if use_hard_softmax:\n",
    "                self.softmax = lambda x: hard_softmax(x, dim=-1)\n",
    "            else:\n",
    "                self.softmax = lambda x: nn.functional.softmax(x, dim=-1)\n",
    "        else:\n",
    "            # self.softmax = lambda x: x / x.size(-1)\n",
    "            self.softmax = normalize_by_sum\n",
    "        self.square = square\n",
    "        self.topk = topk\n",
    "\n",
    "    def __call__(self, func, x, m, t):\n",
    "        xm = torch.cat([x, m.unsqueeze(-1)], dim=-1)\n",
    "        dxdt = func(t, xm)\n",
    "        mean_velo = func.mean_velocity(torch.tensor([t], dtype=x.dtype, device=x.device))\n",
    "        if self.weighted:\n",
    "            masses = self.softmax(m) * m.shape[-1]\n",
    "            if self.detach_m:\n",
    "                masses = masses.detach()\n",
    "            dxdt = dxdt * masses.unsqueeze(-1) # weighted by mass\n",
    "        if self.square:\n",
    "            res = torch.square(dxdt[...,:-1].norm(dim=-1) - mean_velo)\n",
    "        else:\n",
    "            res = torch.abs(dxdt[...,:-1].norm(dim=-1) - mean_velo)\n",
    "        if self.topk is not None:\n",
    "            values, _ = torch.topk(res, self.topk, dim=0, largest=True, sorted=True)\n",
    "            return values.mean()\n",
    "        else:\n",
    "            return res.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
