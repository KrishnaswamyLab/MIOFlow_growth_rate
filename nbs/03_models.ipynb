{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: API details.\n",
    "output-file: models.html\n",
    "title: Models\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| default_exp models\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "import itertools\n",
    "from torch.nn  import functional as F \n",
    "import torch.nn as nn\n",
    "import torch\n",
    "class ToyODE(nn.Module):\n",
    "    \"\"\" \n",
    "    ODE derivative network\n",
    "    \n",
    "    feature_dims (int) default '5': dimension of the inputs, either in ambient space or embedded space.\n",
    "    layer (list of int) defaulf ''[64]'': the hidden layers of the network.\n",
    "    activation (torch.nn) default '\"ReLU\"': activation function applied in between layers.\n",
    "    scales (NoneType|list of float) default 'None': the initial scale for the noise in the trajectories. One scale per bin, add more if using an adaptative ODE solver.\n",
    "    n_aug (int) default '1': number of added dimensions to the input of the network. Total dimensions are features_dim + 1 (time) + n_aug. \n",
    "    \n",
    "    Method\n",
    "    forward (Callable)\n",
    "        forward pass of the ODE derivative network.\n",
    "        Parameters:\n",
    "        t (torch.tensor): time of the evaluation.\n",
    "        x (torch.tensor): position of the evalutation.\n",
    "        Return:\n",
    "        derivative at time t and position x.   \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        feature_dims=5,\n",
    "        layers=[64],\n",
    "        activation='ReLU',\n",
    "        scales=None,\n",
    "        n_aug=2,\n",
    "        zero_gate=False,\n",
    "        time_homogeneous=False,\n",
    "        unif_velo=False,\n",
    "        unif_velo_strict=False,\n",
    "        unif_velo_t=False,\n",
    "        separate_network_m=False,\n",
    "    ):\n",
    "        super(ToyODE, self).__init__()\n",
    "        self.time_homogeneous = time_homogeneous\n",
    "        t_dim = 0 if time_homogeneous else 1\n",
    "        self.separate_network_m = separate_network_m\n",
    "\n",
    "        if separate_network_m: # x does not depend on m, but m does depend on x.\n",
    "            x_feature_dims = feature_dims - 1 # remove the m dim for x seq\n",
    "            steps = [x_feature_dims+t_dim+n_aug, *layers, x_feature_dims]\n",
    "            pairs = zip(steps, steps[1:])\n",
    "            chain = list(itertools.chain(*list(zip(\n",
    "                map(lambda e: nn.Linear(*e), pairs), \n",
    "                itertools.repeat(getattr(nn, activation)())\n",
    "            ))))[:-1]\n",
    "            self.x_seq = nn.Sequential(*chain)\n",
    "\n",
    "            steps = [feature_dims+t_dim+n_aug, *layers, 1]\n",
    "            pairs = zip(steps, steps[1:])\n",
    "            chain = list(itertools.chain(*list(zip(\n",
    "                map(lambda e: nn.Linear(*e), pairs), \n",
    "                itertools.repeat(getattr(nn, activation)())\n",
    "            ))))[:-1]\n",
    "            self.m_seq = nn.Sequential(*chain)\n",
    "\n",
    "            def seq(self, x):\n",
    "                return torch.cat((self.x_seq(x[...,:-1]), self.m_seq(x)), dim=-1)\n",
    "            self.seq = seq.__get__(self)\n",
    "\n",
    "        else:\n",
    "            steps = [feature_dims+t_dim+n_aug, *layers, feature_dims]\n",
    "            pairs = zip(steps, steps[1:])\n",
    "            chain = list(itertools.chain(*list(zip(\n",
    "                map(lambda e: nn.Linear(*e), pairs), \n",
    "                itertools.repeat(getattr(nn, activation)())\n",
    "            ))))[:-1]\n",
    "\n",
    "            # self.chain = chain\n",
    "            self.seq = (nn.Sequential(*chain))\n",
    "\n",
    "        self.alpha = nn.Parameter(torch.tensor(scales, requires_grad=True).float()) if scales is not None else None\n",
    "        self.n_aug = n_aug       \n",
    "        self.zero_gate = zero_gate\n",
    "        \n",
    "        self.unif_velo_strict = unif_velo_strict\n",
    "        if unif_velo_strict:\n",
    "            # assert unif_velo, \"Cannot set unif_velo_strict to True without unif_velo.\"\n",
    "            unif_velo = True\n",
    "        self.unif_velo = unif_velo\n",
    "        self.unif_velo_t = unif_velo_t\n",
    "        if unif_velo_t:\n",
    "            assert unif_velo, \"Cannot set unif_velo_t to True without unif_velo.\"\n",
    "        if unif_velo:\n",
    "            if unif_velo_t:\n",
    "                self.mean_velo_logit_network = nn.Sequential(\n",
    "                    nn.Linear(1, 8), \n",
    "                    getattr(nn, activation)(), \n",
    "                    nn.Linear(8, 8),\n",
    "                    getattr(nn, activation)(),\n",
    "                    nn.Linear(8, 1)\n",
    "                )\n",
    "            else:\n",
    "                self.mean_velo_logit = nn.Parameter(torch.tensor(0.).float(), requires_grad=True)\n",
    "        \n",
    "    def mean_velocity(self, t):\n",
    "        assert self.unif_velo, \"Cannot call mean_velocity without unif_velo.\"\n",
    "        if self.unif_velo_t:\n",
    "            return torch.exp(self.mean_velo_logit_network(t))\n",
    "        return torch.exp(self.mean_velo_logit)\n",
    "\n",
    "    def forward(self, t, x): #NOTE the forward pass when we use torchdiffeq must be forward(self,t,x)\n",
    "        if self.zero_gate:\n",
    "            m = x[...,-1]\n",
    "        zero = torch.tensor([0]).cuda() if x.is_cuda else torch.tensor([0])\n",
    "        zeros = zero.repeat(x.size()[0],self.n_aug)\n",
    "        if self.time_homogeneous:\n",
    "            aug = torch.cat((x,zeros),dim=1)\n",
    "        else:\n",
    "            time = t.repeat(x.size()[0],1)\n",
    "            aug = torch.cat((x,time,zeros),dim=1)\n",
    "        x = self.seq(aug)\n",
    "        if self.alpha is not None:\n",
    "            z = torch.randn(x.size(),requires_grad=False).cuda() if x.is_cuda else torch.randn(x.size(),requires_grad=False)\n",
    "        dxdt = x + z*self.alpha[int(t-1)] if self.alpha is not None else x\n",
    "        if self.zero_gate:\n",
    "            dxdt[m == 0., -1] = 0.\n",
    "\n",
    "        if self.unif_velo_strict:\n",
    "            norms = dxdt[...,:-1].norm(dim=-1, keepdim=True)\n",
    "            dxdt1 = torch.cat((dxdt[...,:-1] / norms * torch.exp(self.mean_velo_logit), dxdt[...,-1].unsqueeze(-1)), dim=-1)\n",
    "            return dxdt1\n",
    "        \n",
    "        return dxdt\n",
    "    \n",
    "    def forward_x(self, t, x): # this x does not include m, so need to concat zeros\n",
    "        x = torch.cat((x, torch.zeros_like(x[..., -1:])), dim=-1)\n",
    "        return self.forward(t, x)[...,:-1]\n",
    "    \n",
    "    def forward_m(self, t, x):\n",
    "        return self.forward(t, x)[...,-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "def make_model(\n",
    "    feature_dims=5,\n",
    "    layers=[64],\n",
    "    # output_dims=5,\n",
    "    activation='ReLU',\n",
    "    which='ode',\n",
    "    method='rk4',\n",
    "    rtol=None,\n",
    "    atol=None,\n",
    "    scales=None,\n",
    "    n_aug=2,\n",
    "    noise_type='diagonal', sde_type='ito',\n",
    "    use_norm=False,\n",
    "    use_cuda=False,\n",
    "    # in_features=2, out_features=2, gunc=None,\n",
    "    m_transform='exp',\n",
    "    dt=0.1,\n",
    "    time_homogeneous=False,\n",
    "    unif_velo=False,\n",
    "    unif_velo_strict=False,\n",
    "    uni_velo_t=False,\n",
    "    separate_network_m=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates the 'ode' model or 'sde' model or the Geodesic Autoencoder. \n",
    "    See the parameters of the respective classes.\n",
    "    \"\"\"\n",
    "    assert use_norm == False, \"This way of energy loss is removed. Now using energy loss.\"\n",
    "    if m_transform == 'exp':\n",
    "        m_trans = torch.exp\n",
    "        m_init = 0.\n",
    "    elif m_transform == 'relu':\n",
    "        m_trans = torch.nn.functional.relu\n",
    "        m_init = 1.\n",
    "    else:\n",
    "        raise ValueError('m_transform must be exp or relu')\n",
    "    if which == 'ode':\n",
    "        ode = ToyODE(feature_dims, layers, activation,scales,n_aug, time_homogeneous=time_homogeneous, unif_velo=unif_velo, unif_velo_strict=unif_velo_strict, unif_velo_t=uni_velo_t, separate_network_m=separate_network_m)\n",
    "        model = ToyModel(ode,method,rtol, atol)\n",
    "    elif which == 'sde':\n",
    "        raise NotImplementedError(\"SDE model not implemented.\")\n",
    "        # ode = ToyODE(feature_dims, layers, activation,scales,n_aug)\n",
    "        # model = ToySDEModel(\n",
    "        #     ode, method, noise_type, sde_type,\n",
    "        #     in_features=in_features, out_features=out_features, gunc=gunc\n",
    "        # )\n",
    "    elif which == 'ode_growth_rate':\n",
    "        # ode = ToyODE(feature_dims + 1, layers, activation,scales,n_aug, zero_gate=True)\n",
    "        ode = ToyODE(feature_dims + 1, layers, activation,scales,n_aug, zero_gate=False, time_homogeneous=time_homogeneous, unif_velo=unif_velo, unif_velo_strict=unif_velo_strict, unif_velo_t=uni_velo_t, separate_network_m=separate_network_m)\n",
    "        model = GrowthRateModel(ode,method,rtol, atol, m_transform=m_trans, m_init=m_init)\n",
    "    elif which == 'sde_growth_rate':\n",
    "        assert method in ['adjoint_reversible_heun', 'euler', 'euler_heun', 'heun', 'log_ode', 'midpoint', 'milstein', 'reversible_heun', 'srk'] # rk4 not supported.\n",
    "        ode = ToyODE(feature_dims + 1, layers, activation,scales,n_aug, zero_gate=True, time_homogeneous=time_homogeneous, unif_velo=unif_velo, unif_velo_strict=unif_velo_strict, unif_velo_t=uni_velo_t, separate_network_m=separate_network_m)\n",
    "        gunc = ToyODE(feature_dims + 1, layers, activation,scales,n_aug, zero_gate=False, time_homogeneous=time_homogeneous, unif_velo=unif_velo, unif_velo_strict=unif_velo_strict, unif_velo_t=uni_velo_t, separate_network_m=separate_network_m)\n",
    "        model = GrowthRateSDEModel(\n",
    "            ode, method, noise_type, sde_type,\n",
    "            in_features=feature_dims + 1, out_features=feature_dims + 1, gunc=gunc, dt=dt, m_transform=m_trans, m_init=m_init,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Model {which} not recognized.\")\n",
    "        # model = ToyGeo(feature_dims, layers, output_dims, activation)\n",
    "    if use_cuda:\n",
    "        model.cuda()\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import itertools\n",
    "import torch.nn as nn\n",
    "from torch.nn  import functional as F \n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    \"\"\" \n",
    "    Geodesic Autoencoder\n",
    "    \n",
    "    encoder_layers (list of int) default '[100, 100, 20]': encoder_layers[0] is the feature dimension, and encoder_layers[-1] the embedded dimension.\n",
    "    decoder_layers (list of int) defaulf '[20, 100, 100]': decoder_layers[0] is the embbeded dim and decoder_layers[-1] the feature dim.\n",
    "    activation (torch.nn) default '\"Tanh\"': activation function applied in between layers.\n",
    "    use_cuda (bool) default to False: Whether to use GPU or CPU.\n",
    "    \n",
    "    Method\n",
    "    encode\n",
    "        forward pass of the encoder\n",
    "        x (torch.tensor): observations\n",
    "        Return:\n",
    "        the encoded observations\n",
    "    decode\n",
    "        forward pass of the decoder\n",
    "        z (torch.tensor): embedded observations\n",
    "        Return:\n",
    "        the decoded observations\n",
    "    forward (Callable):\n",
    "        full forward pass, encoder and decoder\n",
    "        x (torch.tensor): observations\n",
    "        Return:\n",
    "        denoised observations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_layers = [100, 100, 20],\n",
    "        decoder_layers = [20, 100, 100],\n",
    "        activation = 'Tanh',\n",
    "        use_cuda = False\n",
    "    ):        \n",
    "        super(Autoencoder, self).__init__()\n",
    "        if decoder_layers is None:\n",
    "            decoder_layers = [*encoder_layers[::-1]]\n",
    "        device = 'cuda' if use_cuda else 'cpu'\n",
    "        \n",
    "        encoder_shapes = list(zip(encoder_layers, encoder_layers[1:]))\n",
    "        decoder_shapes = list(zip(decoder_layers, decoder_layers[1:]))\n",
    "        \n",
    "        encoder_linear = list(map(lambda a: nn.Linear(*a), encoder_shapes))\n",
    "        decoder_linear = list(map(lambda a: nn.Linear(*a), decoder_shapes))\n",
    "        \n",
    "        encoder_riffle = list(itertools.chain(*zip(encoder_linear, itertools.repeat(getattr(nn, activation)()))))[:-1]\n",
    "        encoder = nn.Sequential(*encoder_riffle).to(device)\n",
    "        \n",
    "        decoder_riffle = list(itertools.chain(*zip(decoder_linear, itertools.repeat(getattr(nn, activation)()))))[:-1]\n",
    "\n",
    "        decoder = nn.Sequential(*decoder_riffle).to(device)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        \n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "adjoint = True\n",
    "if adjoint:\n",
    "    from torchdiffeq import odeint_adjoint as odeint\n",
    "else:\n",
    "    from torchdiffeq import odeint\n",
    "import os, math, numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class ToyModel(nn.Module):\n",
    "    \"\"\" \n",
    "    Neural ODE\n",
    "        func (nn.Module): The network modeling the derivative.\n",
    "        method (str) defaulf '\"rk4\"': any methods from torchdiffeq.\n",
    "        rtol (NoneType | float): the relative tolerance of the ODE solver.\n",
    "        atol (NoneType | float): the absolute tolerance. of the ODE solver.\n",
    "        use_norm (bool): if True keeps the norm of func.\n",
    "        norm (list of torch.tensor): the norm of the derivative.\n",
    "        \n",
    "        Method\n",
    "        forward (Callable)\n",
    "            x (torch.tensor): the initial sample\n",
    "            t (torch.tensor) time points where we suppose x is from t[0]\n",
    "            return the last sample or the whole seq.      \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, func, method='rk4', rtol=None, atol=None, use_norm=False, use_norm_m=False):\n",
    "        super(ToyModel, self).__init__()        \n",
    "        self.func = func\n",
    "        self.method = method\n",
    "        self.rtol=rtol\n",
    "        self.atol=atol\n",
    "        if use_norm or use_norm_m:\n",
    "            raise ValueError(\"use_norm and use_norm_m are no longer used. Now computing the energy penalty is outside the model.\")\n",
    "        # self.use_norm = use_norm\n",
    "        # self.use_norm_m = use_norm_m\n",
    "        # self.norm=[]\n",
    "        # self.norm_m = []\n",
    "\n",
    "    \"\"\"\n",
    "    Xingzhi: changed the energy penalty to being computed outside the model.\n",
    "    \"\"\"\n",
    "    def forward(self, x, t, return_whole_sequence=False):\n",
    "\n",
    "        # if self.use_norm or self.use_norm_m:\n",
    "        #     for time in t: \n",
    "        #         \"\"\"\n",
    "        #         Xingzhi: This looks weird to me. it loops through time but only uses the initial x.\n",
    "        #         I see that it might be an approximation because of the memory burden to save each intermediate x.\n",
    "        #         Besides, putting a list of norms and relying on the outer code (the training loop) to reset it is dangerous, but I'll not change it.\n",
    "        #         \"\"\"\n",
    "        #         dxdt = self.func(time,x)\n",
    "        #         if self.use_norm:\n",
    "        #             self.norm.append(torch.linalg.norm(dxdt).pow(2)) \n",
    "        #         if self.use_norm_m:\n",
    "        #             self.norm_m.append(torch.square(dxdt[...,-1]).mean())\n",
    "\n",
    "        kwargs = {'method': self.method}\n",
    "        if self.atol is not None:\n",
    "            kwargs['atol'] = self.atol\n",
    "        if self.rtol is not None:\n",
    "            kwargs['rtol'] = self.rtol\n",
    "\n",
    "\n",
    "        x = odeint(self.func, x, t, **kwargs)\n",
    "\n",
    "       \n",
    "        x = x[-1] if not return_whole_sequence else x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from torchdiffeq import odeint_adjoint as odeint\n",
    "import os, math, numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchsde\n",
    "\n",
    "class ToySDEModel(nn.Module):\n",
    "    \"\"\" \n",
    "    Neural SDE model\n",
    "        func (nn.Module): drift term.\n",
    "        genc (nn.Module): diffusion term.\n",
    "        method (str): method of the SDE solver.\n",
    "        \n",
    "        Method\n",
    "        forward (Callable)\n",
    "            x (torch.tensor): the initial sample\n",
    "            t (torch.tensor) time points where we suppose x is from t[0]\n",
    "            return the last sample or the whole seq.  \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, func, method='euler', noise_type='diagonal', sde_type='ito', \n",
    "    in_features=2, out_features=2, gunc=None, dt=0.1):\n",
    "        super(ToySDEModel, self).__init__()        \n",
    "        self.func = func\n",
    "        self.method = method\n",
    "        self.noise_type = noise_type\n",
    "        self.sde_type = sde_type\n",
    "        if gunc is None:\n",
    "            self._gunc_args = 'y'\n",
    "            self.gunc = nn.Linear(in_features, out_features)\n",
    "        else:\n",
    "            self._gunc_args = 't,y'\n",
    "            self.gunc = gunc\n",
    "\n",
    "        self.dt = dt\n",
    "        \n",
    "    def f(self, t, y):\n",
    "        return self.func(t, y)\n",
    "\n",
    "    def g(self, t, y):\n",
    "        return self.gunc(t, y) if self._gunc_args == 't,y' else self.gunc(y)\n",
    "        return 0.3 * torch.sigmoid(torch.cos(t) * torch.exp(-y))\n",
    "\n",
    "    def forward(self, x, t, return_whole_sequence=False, dt=None):\n",
    "        dt = self.dt if self.dt is not None else 0.1 if dt is None else dt        \n",
    "        x = torchsde.sdeint(self, x, t, method=self.method, dt=dt)\n",
    "       \n",
    "        x = x[-1] if not return_whole_sequence else x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "if adjoint:\n",
    "    from torchdiffeq import odeint_adjoint as odeint\n",
    "\n",
    "    def find_parameters(module):\n",
    "\n",
    "        assert isinstance(module, nn.Module)\n",
    "\n",
    "        # If called within DataParallel, parameters won't appear in module.parameters().\n",
    "        if getattr(module, '_is_replica', False):\n",
    "\n",
    "            def find_tensor_attributes(module):\n",
    "                tuples = [(k, v) for k, v in module.__dict__.items() if torch.is_tensor(v) and v.requires_grad]\n",
    "                return tuples\n",
    "\n",
    "            gen = module._named_members(get_members_fn=find_tensor_attributes)\n",
    "            return [param for _, param in gen]\n",
    "        else:\n",
    "            return list(module.parameters())\n",
    "\n",
    "else:\n",
    "    from torchdiffeq import odeint\n",
    "\n",
    "import os, math, numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class GrowthRateModel(ToyModel):\n",
    "    \"\"\"\n",
    "    Last feature dim is the growth rate / mass.\n",
    "    \"\"\"\n",
    "    def __init__(self, func, method='rk4', rtol=None, atol=None, use_norm=False, m_init=1., m_transform=lambda x: x):\n",
    "        super().__init__(func, method, rtol, atol, use_norm)\n",
    "        self.m_init = m_init\n",
    "        self.m_transform = m_transform\n",
    "    \n",
    "    def forward(self, x, t, return_whole_sequence=False, m0=None):\n",
    "        if m0 is None:\n",
    "            m0 = torch.full((x.size()[0], 1), self.m_init, dtype=x.dtype, device=x.device)\n",
    "        elif m0.ndim == 1:\n",
    "            m0 = m0.unsqueeze(1)\n",
    "        xm = torch.cat((x,m0),dim=1)\n",
    "        if self.func.separate_network_m:\n",
    "            kwargs = {'method': self.method}\n",
    "            if self.atol is not None:\n",
    "                kwargs['atol'] = self.atol\n",
    "            if self.rtol is not None:\n",
    "                kwargs['rtol'] = self.rtol\n",
    "            m = odeint(self.func, xm, t, **kwargs)[...,-1]\n",
    "            if adjoint:\n",
    "                kwargs['adjoint_params'] = find_parameters(self.func.x_seq)\n",
    "            x = odeint(self.func.forward_x, x, t, **kwargs)\n",
    "            if return_whole_sequence:\n",
    "                return x, self.m_transform(m)\n",
    "            else:\n",
    "                return x[-1], self.m_transform(m)[-1]\n",
    "\n",
    "        else:\n",
    "            x = super().forward(xm, t, return_whole_sequence)\n",
    "            return x[...,:-1], self.m_transform(x[...,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class GrowthRateSDEModel(ToySDEModel):\n",
    "    def __init__(self, func, method='euler', noise_type='diagonal', sde_type='ito',\n",
    "                 in_features=2, out_features=2, gunc=None, dt=0.1, m_init=0., m_transform=lambda x: x):\n",
    "        super().__init__(func, method, noise_type, sde_type, in_features, out_features, gunc, dt)\n",
    "        self.m_init = m_init\n",
    "        self.m_transform = m_transform\n",
    "    \n",
    "    def forward(self, x, t, return_whole_sequence=False, m0=None, dt=None):\n",
    "        if m0 is None:\n",
    "            m0 = torch.full((x.size()[0], 1), self.m_init, dtype=x.dtype, device=x.device)\n",
    "        elif m0.ndim == 1:\n",
    "            m0 = m0.unsqueeze(1)\n",
    "        xm = torch.cat((x,m0),dim=1)\n",
    "        x = super().forward(xm, t, return_whole_sequence, dt)\n",
    "        return x[...,:-1], self.m_transform(x[...,-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
